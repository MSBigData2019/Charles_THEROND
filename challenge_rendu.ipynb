{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Large Scale Machine Learning\n",
    "\n",
    "### Authors: \n",
    "#### Pavlo Mozharovskyi (pavlo.mozharovskyi@telecom-paristech.fr), Umut Şimşekli (umut.simsekli@telecom-paristech.fr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion of algorithms for face recognition\n",
    "\n",
    "The increasingly ubiquitous presence of biometric solutions and face recognition in particular in everyday life requires their adaptation for practical scenario. In the presence of several possible solutions, and if global decisions are to be made, each such single solution can be far less efficient than tailoring them to the complexity of an image.\n",
    "\n",
    "In this challenge, the goal is to build a fusion of algorithms in order to construct the best suited solution for comparison of a pair of images. This fusion will be driven by qualities computed on each image.\n",
    "\n",
    "Comparing of two images is done in two steps. 1st, a vector of features is computed for each image. 2nd, a simple function produces a vector of scores for a pair of images. The goal is to create a function that will compare a pair of images based on the information mentioned above, and decide whether two images belong to the same person.\n",
    "\n",
    "You are provided a label set of training data and a test set without labels. You should submit a .csv file with labels for each entry of this test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The properties of the dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set consist of two files, **xtrain_challenge.csv** and **xtest_challenge.csv**.\n",
    "\n",
    "File **xtrain_challenge.csv** contains one observation per row which contains following entries based on a pair of images, A and B say:\n",
    " * columns 1-14 - 14 qualities on image A;\n",
    " * columns 15-28 - 14 qualities on image B;\n",
    " * columns 29-36 - 8 matching scores between A and B.\n",
    "\n",
    "File **ytrain_challenge.csv** contains one line with each entry corresponding to one observation in **xtrain_challenge.csv**, maintaining the order, and has '1' if a pair of images belong to the same person and '0' otherwise.\n",
    "\n",
    "There are in total 3.196.465 training observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File **xtest_challenge.csv** has the same structure as file **xtrain_challenge.csv**.\n",
    "\n",
    "There are in total 1.598.219 test observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The performance criterion¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance criterion is the **prediction accuracy** on the test set, which is a value between 0 and 1, the higher the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data, input (file **xtrain_challenge.csv**): https://www.dropbox.com/s/myvvtmw61eg5gk7/xtrain_challenge.csv\n",
    "\n",
    "Training data, output (file **ytrain_challenge.csv**): https://www.dropbox.com/s/cleumxob0dfzre4/ytrain_challenge.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data, input (file **xtest_challenge.csv**): https://www.dropbox.com/s/bfrx8b4mqythm4q/xtest_challenge.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and investigate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decouverte du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.read_csv('xtrain_challenge.csv')\n",
    "ytrain = pd.read_csv('ytrain_challenge.csv')\n",
    "dataset=xtrain.merge(ytrain,  left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fA1</th>\n",
       "      <th>fA2</th>\n",
       "      <th>fA3</th>\n",
       "      <th>fA4</th>\n",
       "      <th>fA5</th>\n",
       "      <th>fA6</th>\n",
       "      <th>fA7</th>\n",
       "      <th>fA8</th>\n",
       "      <th>fA9</th>\n",
       "      <th>fA10</th>\n",
       "      <th>...</th>\n",
       "      <th>fB14</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "      <td>3.196465e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.123656e+00</td>\n",
       "      <td>7.200140e-01</td>\n",
       "      <td>8.895844e-02</td>\n",
       "      <td>2.938227e+01</td>\n",
       "      <td>9.112188e-01</td>\n",
       "      <td>4.165448e-03</td>\n",
       "      <td>8.456615e-02</td>\n",
       "      <td>1.323673e-01</td>\n",
       "      <td>2.970159e-02</td>\n",
       "      <td>-2.833317e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.477718e+02</td>\n",
       "      <td>3.874350e+03</td>\n",
       "      <td>2.434998e+03</td>\n",
       "      <td>4.200151e+03</td>\n",
       "      <td>4.242067e+03</td>\n",
       "      <td>3.488791e+03</td>\n",
       "      <td>3.399066e+03</td>\n",
       "      <td>3.509614e+03</td>\n",
       "      <td>3.745971e+03</td>\n",
       "      <td>2.857347e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.618458e+00</td>\n",
       "      <td>4.224322e-01</td>\n",
       "      <td>2.824698e-01</td>\n",
       "      <td>9.793075e+00</td>\n",
       "      <td>2.757797e-01</td>\n",
       "      <td>4.433567e-02</td>\n",
       "      <td>2.729676e-01</td>\n",
       "      <td>4.159110e-01</td>\n",
       "      <td>1.279389e-01</td>\n",
       "      <td>2.096133e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.221232e+02</td>\n",
       "      <td>2.063822e+03</td>\n",
       "      <td>2.272729e+02</td>\n",
       "      <td>2.954367e+03</td>\n",
       "      <td>2.127659e+03</td>\n",
       "      <td>1.360081e+03</td>\n",
       "      <td>1.363354e+03</td>\n",
       "      <td>1.066627e+03</td>\n",
       "      <td>1.548290e+03</td>\n",
       "      <td>4.517637e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.500000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.550000e+00</td>\n",
       "      <td>-3.030000e+00</td>\n",
       "      <td>-3.000000e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.087900e+02</td>\n",
       "      <td>1.212700e+03</td>\n",
       "      <td>1.511400e+03</td>\n",
       "      <td>7.218000e+02</td>\n",
       "      <td>1.133800e+03</td>\n",
       "      <td>6.682000e+02</td>\n",
       "      <td>7.200000e+02</td>\n",
       "      <td>1.358000e+03</td>\n",
       "      <td>7.107000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.950000e+00</td>\n",
       "      <td>2.500000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.000000e-02</td>\n",
       "      <td>-2.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214200e+02</td>\n",
       "      <td>2.420300e+03</td>\n",
       "      <td>2.296300e+03</td>\n",
       "      <td>2.086200e+03</td>\n",
       "      <td>2.735400e+03</td>\n",
       "      <td>2.457200e+03</td>\n",
       "      <td>2.335000e+03</td>\n",
       "      <td>2.742600e+03</td>\n",
       "      <td>2.620100e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.390000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.766400e+02</td>\n",
       "      <td>3.062600e+03</td>\n",
       "      <td>2.439800e+03</td>\n",
       "      <td>3.073300e+03</td>\n",
       "      <td>3.402000e+03</td>\n",
       "      <td>3.063300e+03</td>\n",
       "      <td>2.999800e+03</td>\n",
       "      <td>3.151800e+03</td>\n",
       "      <td>3.218800e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.380000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>8.000000e-02</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>3.216600e+02</td>\n",
       "      <td>4.442500e+03</td>\n",
       "      <td>2.589300e+03</td>\n",
       "      <td>5.143800e+03</td>\n",
       "      <td>4.898800e+03</td>\n",
       "      <td>4.406800e+03</td>\n",
       "      <td>4.375500e+03</td>\n",
       "      <td>3.926300e+03</td>\n",
       "      <td>4.396900e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.940000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.400000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>7.600000e-01</td>\n",
       "      <td>2.600000e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.814200e+02</td>\n",
       "      <td>1.204420e+04</td>\n",
       "      <td>3.035900e+03</td>\n",
       "      <td>1.666650e+04</td>\n",
       "      <td>1.181290e+04</td>\n",
       "      <td>7.731000e+03</td>\n",
       "      <td>7.580100e+03</td>\n",
       "      <td>6.949500e+03</td>\n",
       "      <td>8.524900e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                fA1           fA2           fA3           fA4           fA5  \\\n",
       "count  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06   \n",
       "mean   3.123656e+00  7.200140e-01  8.895844e-02  2.938227e+01  9.112188e-01   \n",
       "std    1.618458e+00  4.224322e-01  2.824698e-01  9.793075e+00  2.757797e-01   \n",
       "min   -4.500000e-01  0.000000e+00  0.000000e+00  1.800000e+01  0.000000e+00   \n",
       "25%    1.950000e+00  2.500000e-01  0.000000e+00  2.200000e+01  1.000000e+00   \n",
       "50%    3.390000e+00  1.000000e+00  0.000000e+00  2.600000e+01  1.000000e+00   \n",
       "75%    4.380000e+00  1.000000e+00  0.000000e+00  3.400000e+01  1.000000e+00   \n",
       "max    7.940000e+00  1.000000e+00  1.000000e+00  7.400000e+01  1.000000e+00   \n",
       "\n",
       "                fA6           fA7           fA8           fA9          fA10  \\\n",
       "count  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06   \n",
       "mean   4.165448e-03  8.456615e-02  1.323673e-01  2.970159e-02 -2.833317e-04   \n",
       "std    4.433567e-02  2.729676e-01  4.159110e-01  1.279389e-01  2.096133e-02   \n",
       "min    0.000000e+00  0.000000e+00 -2.550000e+00 -3.030000e+00 -3.000000e-01   \n",
       "25%    0.000000e+00  0.000000e+00 -2.000000e-02 -2.000000e-02  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  2.000000e-02  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  1.000000e-01  8.000000e-02  1.000000e-02   \n",
       "max    1.000000e+00  1.000000e+00  2.500000e+00  7.600000e-01  2.600000e-01   \n",
       "\n",
       "           ...               fB14            s1            s2            s3  \\\n",
       "count      ...       3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06   \n",
       "mean       ...       2.477718e+02  3.874350e+03  2.434998e+03  4.200151e+03   \n",
       "std        ...       1.221232e+02  2.063822e+03  2.272729e+02  2.954367e+03   \n",
       "min        ...      -3.087900e+02  1.212700e+03  1.511400e+03  7.218000e+02   \n",
       "25%        ...       2.214200e+02  2.420300e+03  2.296300e+03  2.086200e+03   \n",
       "50%        ...       2.766400e+02  3.062600e+03  2.439800e+03  3.073300e+03   \n",
       "75%        ...       3.216600e+02  4.442500e+03  2.589300e+03  5.143800e+03   \n",
       "max        ...       4.814200e+02  1.204420e+04  3.035900e+03  1.666650e+04   \n",
       "\n",
       "                 s4            s5            s6            s7            s8  \\\n",
       "count  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06  3.196465e+06   \n",
       "mean   4.242067e+03  3.488791e+03  3.399066e+03  3.509614e+03  3.745971e+03   \n",
       "std    2.127659e+03  1.360081e+03  1.363354e+03  1.066627e+03  1.548290e+03   \n",
       "min    1.133800e+03  6.682000e+02  7.200000e+02  1.358000e+03  7.107000e+02   \n",
       "25%    2.735400e+03  2.457200e+03  2.335000e+03  2.742600e+03  2.620100e+03   \n",
       "50%    3.402000e+03  3.063300e+03  2.999800e+03  3.151800e+03  3.218800e+03   \n",
       "75%    4.898800e+03  4.406800e+03  4.375500e+03  3.926300e+03  4.396900e+03   \n",
       "max    1.181290e+04  7.731000e+03  7.580100e+03  6.949500e+03  8.524900e+03   \n",
       "\n",
       "                  y  \n",
       "count  3.196465e+06  \n",
       "mean   2.857347e-01  \n",
       "std    4.517637e-01  \n",
       "min    0.000000e+00  \n",
       "25%    0.000000e+00  \n",
       "50%    0.000000e+00  \n",
       "75%    1.000000e+00  \n",
       "max    1.000000e+00  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fA1', 'fA2', 'fA3', 'fA4', 'fA5', 'fA6', 'fA7', 'fA8', 'fA9', 'fA10',\n",
      "       'fA11', 'fA12', 'fA13', 'fA14', 'fB1', 'fB2', 'fB3', 'fB4', 'fB5',\n",
      "       'fB6', 'fB7', 'fB8', 'fB9', 'fB10', 'fB11', 'fB12', 'fB13', 'fB14',\n",
      "       's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 'y'],\n",
      "      dtype='object')\n",
      "fA1     float64\n",
      "fA2     float64\n",
      "fA3     float64\n",
      "fA4       int64\n",
      "fA5     float64\n",
      "fA6     float64\n",
      "fA7     float64\n",
      "fA8     float64\n",
      "fA9     float64\n",
      "fA10    float64\n",
      "fA11    float64\n",
      "fA12    float64\n",
      "fA13    float64\n",
      "fA14    float64\n",
      "fB1     float64\n",
      "fB2     float64\n",
      "fB3     float64\n",
      "fB4       int64\n",
      "fB5     float64\n",
      "fB6     float64\n",
      "fB7     float64\n",
      "fB8     float64\n",
      "fB9     float64\n",
      "fB10    float64\n",
      "fB11    float64\n",
      "fB12    float64\n",
      "fB13    float64\n",
      "fB14    float64\n",
      "s1      float64\n",
      "s2      float64\n",
      "s3      float64\n",
      "s4      float64\n",
      "s5      float64\n",
      "s6      float64\n",
      "s7      float64\n",
      "s8      float64\n",
      "y         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset.columns)\n",
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fA1</th>\n",
       "      <td>0.302163</td>\n",
       "      <td>0.175194</td>\n",
       "      <td>0.175699</td>\n",
       "      <td>0.319515</td>\n",
       "      <td>0.113217</td>\n",
       "      <td>0.072562</td>\n",
       "      <td>0.221904</td>\n",
       "      <td>0.328958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA2</th>\n",
       "      <td>0.195161</td>\n",
       "      <td>0.094589</td>\n",
       "      <td>0.108425</td>\n",
       "      <td>0.199246</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.065890</td>\n",
       "      <td>0.127893</td>\n",
       "      <td>0.207402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA3</th>\n",
       "      <td>-0.156674</td>\n",
       "      <td>-0.077087</td>\n",
       "      <td>-0.057989</td>\n",
       "      <td>-0.170331</td>\n",
       "      <td>-0.014477</td>\n",
       "      <td>0.014819</td>\n",
       "      <td>-0.092125</td>\n",
       "      <td>-0.190083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA4</th>\n",
       "      <td>-0.221524</td>\n",
       "      <td>-0.171908</td>\n",
       "      <td>-0.180561</td>\n",
       "      <td>-0.221287</td>\n",
       "      <td>-0.170143</td>\n",
       "      <td>-0.141654</td>\n",
       "      <td>-0.210691</td>\n",
       "      <td>-0.223554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA5</th>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>0.011938</td>\n",
       "      <td>0.299303</td>\n",
       "      <td>-0.108870</td>\n",
       "      <td>-0.170258</td>\n",
       "      <td>0.090483</td>\n",
       "      <td>0.339884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA6</th>\n",
       "      <td>-0.031351</td>\n",
       "      <td>-0.070376</td>\n",
       "      <td>-0.037729</td>\n",
       "      <td>-0.029526</td>\n",
       "      <td>-0.030271</td>\n",
       "      <td>-0.032501</td>\n",
       "      <td>-0.038793</td>\n",
       "      <td>-0.028510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA7</th>\n",
       "      <td>-0.265017</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>-0.005890</td>\n",
       "      <td>-0.297546</td>\n",
       "      <td>0.114937</td>\n",
       "      <td>0.177315</td>\n",
       "      <td>-0.085074</td>\n",
       "      <td>-0.338711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA8</th>\n",
       "      <td>-0.217639</td>\n",
       "      <td>0.007184</td>\n",
       "      <td>0.018990</td>\n",
       "      <td>-0.247403</td>\n",
       "      <td>0.132473</td>\n",
       "      <td>0.180096</td>\n",
       "      <td>-0.053877</td>\n",
       "      <td>-0.284671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA9</th>\n",
       "      <td>0.053327</td>\n",
       "      <td>0.074944</td>\n",
       "      <td>0.032096</td>\n",
       "      <td>0.055311</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>0.026823</td>\n",
       "      <td>0.047415</td>\n",
       "      <td>0.054904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA10</th>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>-0.001799</td>\n",
       "      <td>-0.003317</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>0.008699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA11</th>\n",
       "      <td>-0.140542</td>\n",
       "      <td>-0.171262</td>\n",
       "      <td>-0.122434</td>\n",
       "      <td>-0.139399</td>\n",
       "      <td>-0.113363</td>\n",
       "      <td>-0.123340</td>\n",
       "      <td>-0.140482</td>\n",
       "      <td>-0.136408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA12</th>\n",
       "      <td>0.191495</td>\n",
       "      <td>0.099302</td>\n",
       "      <td>0.109256</td>\n",
       "      <td>0.198648</td>\n",
       "      <td>0.077493</td>\n",
       "      <td>0.060912</td>\n",
       "      <td>0.143083</td>\n",
       "      <td>0.205547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA13</th>\n",
       "      <td>-0.066403</td>\n",
       "      <td>-0.071074</td>\n",
       "      <td>-0.057936</td>\n",
       "      <td>-0.063700</td>\n",
       "      <td>-0.056268</td>\n",
       "      <td>-0.050618</td>\n",
       "      <td>-0.058940</td>\n",
       "      <td>-0.065783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fA14</th>\n",
       "      <td>0.328132</td>\n",
       "      <td>0.099127</td>\n",
       "      <td>0.111807</td>\n",
       "      <td>0.355814</td>\n",
       "      <td>-0.002474</td>\n",
       "      <td>-0.057820</td>\n",
       "      <td>0.180588</td>\n",
       "      <td>0.383321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB1</th>\n",
       "      <td>0.260167</td>\n",
       "      <td>0.107483</td>\n",
       "      <td>0.129408</td>\n",
       "      <td>0.276053</td>\n",
       "      <td>0.062596</td>\n",
       "      <td>0.027567</td>\n",
       "      <td>0.172888</td>\n",
       "      <td>0.284304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB2</th>\n",
       "      <td>0.188431</td>\n",
       "      <td>0.095472</td>\n",
       "      <td>0.107250</td>\n",
       "      <td>0.193564</td>\n",
       "      <td>0.087638</td>\n",
       "      <td>0.072615</td>\n",
       "      <td>0.126515</td>\n",
       "      <td>0.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB3</th>\n",
       "      <td>-0.165866</td>\n",
       "      <td>-0.074878</td>\n",
       "      <td>-0.049396</td>\n",
       "      <td>-0.180436</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.034551</td>\n",
       "      <td>-0.092143</td>\n",
       "      <td>-0.203643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB4</th>\n",
       "      <td>-0.198834</td>\n",
       "      <td>-0.164619</td>\n",
       "      <td>-0.154750</td>\n",
       "      <td>-0.197262</td>\n",
       "      <td>-0.141450</td>\n",
       "      <td>-0.109586</td>\n",
       "      <td>-0.183613</td>\n",
       "      <td>-0.196707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB5</th>\n",
       "      <td>0.264917</td>\n",
       "      <td>-0.006673</td>\n",
       "      <td>0.006642</td>\n",
       "      <td>0.297212</td>\n",
       "      <td>-0.114936</td>\n",
       "      <td>-0.176417</td>\n",
       "      <td>0.085094</td>\n",
       "      <td>0.338134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB6</th>\n",
       "      <td>-0.205016</td>\n",
       "      <td>0.053923</td>\n",
       "      <td>-0.046857</td>\n",
       "      <td>-0.231785</td>\n",
       "      <td>-0.012357</td>\n",
       "      <td>0.115645</td>\n",
       "      <td>-0.087854</td>\n",
       "      <td>-0.261586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB7</th>\n",
       "      <td>-0.158976</td>\n",
       "      <td>-0.053655</td>\n",
       "      <td>0.045410</td>\n",
       "      <td>-0.176256</td>\n",
       "      <td>0.188875</td>\n",
       "      <td>0.130612</td>\n",
       "      <td>-0.025012</td>\n",
       "      <td>-0.203029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB8</th>\n",
       "      <td>0.055712</td>\n",
       "      <td>-0.063843</td>\n",
       "      <td>0.076263</td>\n",
       "      <td>0.062419</td>\n",
       "      <td>0.148658</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.059092</td>\n",
       "      <td>0.065086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB9</th>\n",
       "      <td>0.070153</td>\n",
       "      <td>0.097732</td>\n",
       "      <td>0.058514</td>\n",
       "      <td>0.071322</td>\n",
       "      <td>0.054919</td>\n",
       "      <td>0.066393</td>\n",
       "      <td>0.071247</td>\n",
       "      <td>0.068630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB10</th>\n",
       "      <td>-0.005989</td>\n",
       "      <td>0.030449</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>-0.006807</td>\n",
       "      <td>-0.001798</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.002284</td>\n",
       "      <td>-0.006046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB11</th>\n",
       "      <td>-0.129855</td>\n",
       "      <td>-0.138497</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.127830</td>\n",
       "      <td>-0.105490</td>\n",
       "      <td>-0.121859</td>\n",
       "      <td>-0.133041</td>\n",
       "      <td>-0.122093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB12</th>\n",
       "      <td>0.118317</td>\n",
       "      <td>0.097883</td>\n",
       "      <td>0.088223</td>\n",
       "      <td>0.115963</td>\n",
       "      <td>0.063006</td>\n",
       "      <td>0.089147</td>\n",
       "      <td>0.107738</td>\n",
       "      <td>0.114966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB13</th>\n",
       "      <td>-0.072326</td>\n",
       "      <td>-0.028683</td>\n",
       "      <td>-0.041971</td>\n",
       "      <td>-0.073486</td>\n",
       "      <td>-0.039717</td>\n",
       "      <td>-0.015032</td>\n",
       "      <td>-0.046142</td>\n",
       "      <td>-0.077432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fB14</th>\n",
       "      <td>0.301710</td>\n",
       "      <td>0.024366</td>\n",
       "      <td>0.076453</td>\n",
       "      <td>0.330833</td>\n",
       "      <td>-0.038822</td>\n",
       "      <td>-0.093554</td>\n",
       "      <td>0.143351</td>\n",
       "      <td>0.357683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414356</td>\n",
       "      <td>0.906090</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.840738</td>\n",
       "      <td>0.821877</td>\n",
       "      <td>0.934036</td>\n",
       "      <td>0.966084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <td>0.414356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.413938</td>\n",
       "      <td>0.399978</td>\n",
       "      <td>0.395905</td>\n",
       "      <td>0.404404</td>\n",
       "      <td>0.435995</td>\n",
       "      <td>0.387498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>0.906090</td>\n",
       "      <td>0.413938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892821</td>\n",
       "      <td>0.905145</td>\n",
       "      <td>0.893168</td>\n",
       "      <td>0.922266</td>\n",
       "      <td>0.870812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s4</th>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.399978</td>\n",
       "      <td>0.892821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.828183</td>\n",
       "      <td>0.814016</td>\n",
       "      <td>0.930177</td>\n",
       "      <td>0.974623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s5</th>\n",
       "      <td>0.840738</td>\n",
       "      <td>0.395905</td>\n",
       "      <td>0.905145</td>\n",
       "      <td>0.828183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.876150</td>\n",
       "      <td>0.893257</td>\n",
       "      <td>0.809908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s6</th>\n",
       "      <td>0.821877</td>\n",
       "      <td>0.404404</td>\n",
       "      <td>0.893168</td>\n",
       "      <td>0.814016</td>\n",
       "      <td>0.876150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881370</td>\n",
       "      <td>0.790388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s7</th>\n",
       "      <td>0.934036</td>\n",
       "      <td>0.435995</td>\n",
       "      <td>0.922266</td>\n",
       "      <td>0.930177</td>\n",
       "      <td>0.893257</td>\n",
       "      <td>0.881370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s8</th>\n",
       "      <td>0.966084</td>\n",
       "      <td>0.387498</td>\n",
       "      <td>0.870812</td>\n",
       "      <td>0.974623</td>\n",
       "      <td>0.809908</td>\n",
       "      <td>0.790388</td>\n",
       "      <td>0.917869</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.801645</td>\n",
       "      <td>0.325149</td>\n",
       "      <td>0.771318</td>\n",
       "      <td>0.804455</td>\n",
       "      <td>0.761075</td>\n",
       "      <td>0.771061</td>\n",
       "      <td>0.809114</td>\n",
       "      <td>0.795365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            s1        s2        s3        s4        s5        s6        s7  \\\n",
       "fA1   0.302163  0.175194  0.175699  0.319515  0.113217  0.072562  0.221904   \n",
       "fA2   0.195161  0.094589  0.108425  0.199246  0.085430  0.065890  0.127893   \n",
       "fA3  -0.156674 -0.077087 -0.057989 -0.170331 -0.014477  0.014819 -0.092125   \n",
       "fA4  -0.221524 -0.171908 -0.180561 -0.221287 -0.170143 -0.141654 -0.210691   \n",
       "fA5   0.267400  0.005037  0.011938  0.299303 -0.108870 -0.170258  0.090483   \n",
       "fA6  -0.031351 -0.070376 -0.037729 -0.029526 -0.030271 -0.032501 -0.038793   \n",
       "fA7  -0.265017  0.006394 -0.005890 -0.297546  0.114937  0.177315 -0.085074   \n",
       "fA8  -0.217639  0.007184  0.018990 -0.247403  0.132473  0.180096 -0.053877   \n",
       "fA9   0.053327  0.074944  0.032096  0.055311  0.025940  0.026823  0.047415   \n",
       "fA10  0.005469  0.025288  0.002957  0.006599 -0.001799 -0.003317  0.004837   \n",
       "fA11 -0.140542 -0.171262 -0.122434 -0.139399 -0.113363 -0.123340 -0.140482   \n",
       "fA12  0.191495  0.099302  0.109256  0.198648  0.077493  0.060912  0.143083   \n",
       "fA13 -0.066403 -0.071074 -0.057936 -0.063700 -0.056268 -0.050618 -0.058940   \n",
       "fA14  0.328132  0.099127  0.111807  0.355814 -0.002474 -0.057820  0.180588   \n",
       "fB1   0.260167  0.107483  0.129408  0.276053  0.062596  0.027567  0.172888   \n",
       "fB2   0.188431  0.095472  0.107250  0.193564  0.087638  0.072615  0.126515   \n",
       "fB3  -0.165866 -0.074878 -0.049396 -0.180436  0.002215  0.034551 -0.092143   \n",
       "fB4  -0.198834 -0.164619 -0.154750 -0.197262 -0.141450 -0.109586 -0.183613   \n",
       "fB5   0.264917 -0.006673  0.006642  0.297212 -0.114936 -0.176417  0.085094   \n",
       "fB6  -0.205016  0.053923 -0.046857 -0.231785 -0.012357  0.115645 -0.087854   \n",
       "fB7  -0.158976 -0.053655  0.045410 -0.176256  0.188875  0.130612 -0.025012   \n",
       "fB8   0.055712 -0.063843  0.076263  0.062419  0.148658  0.021025  0.059092   \n",
       "fB9   0.070153  0.097732  0.058514  0.071322  0.054919  0.066393  0.071247   \n",
       "fB10 -0.005989  0.030449 -0.000511 -0.006807 -0.001798 -0.000639 -0.002284   \n",
       "fB11 -0.129855 -0.138497 -0.114404 -0.127830 -0.105490 -0.121859 -0.133041   \n",
       "fB12  0.118317  0.097883  0.088223  0.115963  0.063006  0.089147  0.107738   \n",
       "fB13 -0.072326 -0.028683 -0.041971 -0.073486 -0.039717 -0.015032 -0.046142   \n",
       "fB14  0.301710  0.024366  0.076453  0.330833 -0.038822 -0.093554  0.143351   \n",
       "s1    1.000000  0.414356  0.906090  0.973016  0.840738  0.821877  0.934036   \n",
       "s2    0.414356  1.000000  0.413938  0.399978  0.395905  0.404404  0.435995   \n",
       "s3    0.906090  0.413938  1.000000  0.892821  0.905145  0.893168  0.922266   \n",
       "s4    0.973016  0.399978  0.892821  1.000000  0.828183  0.814016  0.930177   \n",
       "s5    0.840738  0.395905  0.905145  0.828183  1.000000  0.876150  0.893257   \n",
       "s6    0.821877  0.404404  0.893168  0.814016  0.876150  1.000000  0.881370   \n",
       "s7    0.934036  0.435995  0.922266  0.930177  0.893257  0.881370  1.000000   \n",
       "s8    0.966084  0.387498  0.870812  0.974623  0.809908  0.790388  0.917869   \n",
       "y     0.801645  0.325149  0.771318  0.804455  0.761075  0.771061  0.809114   \n",
       "\n",
       "            s8  \n",
       "fA1   0.328958  \n",
       "fA2   0.207402  \n",
       "fA3  -0.190083  \n",
       "fA4  -0.223554  \n",
       "fA5   0.339884  \n",
       "fA6  -0.028510  \n",
       "fA7  -0.338711  \n",
       "fA8  -0.284671  \n",
       "fA9   0.054904  \n",
       "fA10  0.008699  \n",
       "fA11 -0.136408  \n",
       "fA12  0.205547  \n",
       "fA13 -0.065783  \n",
       "fA14  0.383321  \n",
       "fB1   0.284304  \n",
       "fB2   0.199900  \n",
       "fB3  -0.203643  \n",
       "fB4  -0.196707  \n",
       "fB5   0.338134  \n",
       "fB6  -0.261586  \n",
       "fB7  -0.203029  \n",
       "fB8   0.065086  \n",
       "fB9   0.068630  \n",
       "fB10 -0.006046  \n",
       "fB11 -0.122093  \n",
       "fB12  0.114966  \n",
       "fB13 -0.077432  \n",
       "fB14  0.357683  \n",
       "s1    0.966084  \n",
       "s2    0.387498  \n",
       "s3    0.870812  \n",
       "s4    0.974623  \n",
       "s5    0.809908  \n",
       "s6    0.790388  \n",
       "s7    0.917869  \n",
       "s8    1.000000  \n",
       "y     0.795365  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.corr()[['s1','s2','s3','s4','s5','s6','s7','s8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fA1     0.172994\n",
       "fA2     0.111962\n",
       "fA3    -0.032684\n",
       "fA4    -0.206904\n",
       "fA5     0.013986\n",
       "fA6    -0.000138\n",
       "fA7    -0.014096\n",
       "fA8     0.001426\n",
       "fA9     0.050191\n",
       "fA10   -0.011532\n",
       "fA11   -0.127774\n",
       "fA12    0.129836\n",
       "fA13   -0.043042\n",
       "fA14    0.093309\n",
       "fB1     0.149043\n",
       "fB2     0.114148\n",
       "fB3    -0.022911\n",
       "fB4    -0.190803\n",
       "fB5     0.012304\n",
       "fB6     0.047056\n",
       "fB7    -0.074319\n",
       "fB8    -0.055535\n",
       "fB9     0.081903\n",
       "fB10   -0.010141\n",
       "fB11   -0.127597\n",
       "fB12    0.136991\n",
       "fB13   -0.030770\n",
       "fB14    0.071156\n",
       "s1      0.801645\n",
       "s2      0.325149\n",
       "s3      0.771318\n",
       "s4      0.804455\n",
       "s5      0.761075\n",
       "s6      0.771061\n",
       "s7      0.809114\n",
       "s8      0.795365\n",
       "y       1.000000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.corr()['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que le matching ( colonne s1 à s8) est fortement corrèlé avec le matching des variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2283124\n",
       "1     913341\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que la distribution des variables à prédire n'est pas également répartie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split des données\n",
    "\n",
    "On a vu précédemment que la classe à prédire n'est pas également répartie il faut donc en tenir compte lors du split pour ne pas accroittre cette disparité. Le parametre `stratify` est prévu à cet effet.  \n",
    "Dans le cas d'utilisation de `réseau de neuronnes` et de `light GBM`, Centrer-réduire les données à un réel impact  sur les perfmances des modèles. Cette étape a été réalisée plus bas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,test=train_test_split(dataset,test_size=0.1,stratify=dataset['y'])\n",
    "data=pd.read_csv('dataNormalized.csv',index_col=0)\n",
    "trainN,testN=train_test_split(data,test_size=0.1,stratify=data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### Choix des modèles\n",
    "\n",
    "Suite à une courte recherche bibliographique j'ai décidé de me tourner vers 5 modèles différents:   \n",
    " * LDA\n",
    " * SVM\n",
    " * Réseaux de neuronnes  \n",
    " * XGBoost\n",
    " * LightGBoost\n",
    " \n",
    "La `SVM` et la `LDA` n'ont pas offert assez bon résultat face aux autres mdèles pour justifier de détailler leur implémentation.\n",
    "\n",
    "La suite de mon rapport rendra compte de l'implémentation d'un réseau de neuronnes et des modèles de gradient boosting\n",
    "\n",
    "### Puissance de calcul pour l'optimisation des hyper-parametres\n",
    "\n",
    "Afin d'optimiser le temps de calcul pour trouver les hyperparametres des trois modèles restants, les calculs ont été distribué sur deux clusters dask composé des machines de l'école. Un notebook pour chaque modèle a aussi été mis en place pour pouvoir lancer les calculs simultanéments.\n",
    "\n",
    "Voici le lien vers chacun de ces notebooks:  \n",
    " * Neural network\n",
    " * XGBoost\n",
    " * LGBoost\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats du LGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_num_leaves</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98.779547</td>\n",
       "      <td>0.946638</td>\n",
       "      <td>14.711493</td>\n",
       "      <td>0.277883</td>\n",
       "      <td>0.02</td>\n",
       "      <td>256</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 256}</td>\n",
       "      <td>0.987912</td>\n",
       "      <td>0.987832</td>\n",
       "      <td>0.987872</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990968</td>\n",
       "      <td>0.991069</td>\n",
       "      <td>0.991019</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94.529286</td>\n",
       "      <td>0.926894</td>\n",
       "      <td>13.868437</td>\n",
       "      <td>0.247973</td>\n",
       "      <td>0.02</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 128}</td>\n",
       "      <td>0.986516</td>\n",
       "      <td>0.986479</td>\n",
       "      <td>0.986498</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>2</td>\n",
       "      <td>0.988014</td>\n",
       "      <td>0.988184</td>\n",
       "      <td>0.988099</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.146726</td>\n",
       "      <td>0.511424</td>\n",
       "      <td>11.636487</td>\n",
       "      <td>0.263768</td>\n",
       "      <td>0.02</td>\n",
       "      <td>64</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 64}</td>\n",
       "      <td>0.985063</td>\n",
       "      <td>0.984973</td>\n",
       "      <td>0.985018</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>3</td>\n",
       "      <td>0.985731</td>\n",
       "      <td>0.985950</td>\n",
       "      <td>0.985840</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58.402427</td>\n",
       "      <td>4.384872</td>\n",
       "      <td>9.723119</td>\n",
       "      <td>0.349699</td>\n",
       "      <td>0.02</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.02, 'num_leaves': 32}</td>\n",
       "      <td>0.983464</td>\n",
       "      <td>0.983290</td>\n",
       "      <td>0.983377</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>4</td>\n",
       "      <td>0.983764</td>\n",
       "      <td>0.983920</td>\n",
       "      <td>0.983842</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.826629</td>\n",
       "      <td>0.630166</td>\n",
       "      <td>7.994866</td>\n",
       "      <td>0.201128</td>\n",
       "      <td>0.40</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.4, 'num_leaves': 32}</td>\n",
       "      <td>0.983425</td>\n",
       "      <td>0.983252</td>\n",
       "      <td>0.983339</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>5</td>\n",
       "      <td>0.984965</td>\n",
       "      <td>0.984797</td>\n",
       "      <td>0.984881</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26.649774</td>\n",
       "      <td>0.406546</td>\n",
       "      <td>8.610677</td>\n",
       "      <td>0.083260</td>\n",
       "      <td>0.70</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.7, 'num_leaves': 32}</td>\n",
       "      <td>0.981956</td>\n",
       "      <td>0.981920</td>\n",
       "      <td>0.981938</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>6</td>\n",
       "      <td>0.983401</td>\n",
       "      <td>0.983681</td>\n",
       "      <td>0.983541</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35.504586</td>\n",
       "      <td>0.599904</td>\n",
       "      <td>10.101779</td>\n",
       "      <td>0.123615</td>\n",
       "      <td>0.40</td>\n",
       "      <td>64</td>\n",
       "      <td>{'learning_rate': 0.4, 'num_leaves': 64}</td>\n",
       "      <td>0.980784</td>\n",
       "      <td>0.981377</td>\n",
       "      <td>0.981080</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>7</td>\n",
       "      <td>0.982666</td>\n",
       "      <td>0.983824</td>\n",
       "      <td>0.983245</td>\n",
       "      <td>0.000579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34.976297</td>\n",
       "      <td>0.386149</td>\n",
       "      <td>10.287855</td>\n",
       "      <td>0.128522</td>\n",
       "      <td>0.70</td>\n",
       "      <td>64</td>\n",
       "      <td>{'learning_rate': 0.7, 'num_leaves': 64}</td>\n",
       "      <td>0.979862</td>\n",
       "      <td>0.980125</td>\n",
       "      <td>0.979993</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>8</td>\n",
       "      <td>0.981306</td>\n",
       "      <td>0.981954</td>\n",
       "      <td>0.981630</td>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50.079604</td>\n",
       "      <td>1.605808</td>\n",
       "      <td>12.735095</td>\n",
       "      <td>0.204151</td>\n",
       "      <td>0.40</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.4, 'num_leaves': 128}</td>\n",
       "      <td>0.976314</td>\n",
       "      <td>0.979722</td>\n",
       "      <td>0.978018</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>9</td>\n",
       "      <td>0.979315</td>\n",
       "      <td>0.982140</td>\n",
       "      <td>0.980728</td>\n",
       "      <td>0.001412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70.000215</td>\n",
       "      <td>1.332182</td>\n",
       "      <td>16.317688</td>\n",
       "      <td>0.151828</td>\n",
       "      <td>0.40</td>\n",
       "      <td>256</td>\n",
       "      <td>{'learning_rate': 0.4, 'num_leaves': 256}</td>\n",
       "      <td>0.968426</td>\n",
       "      <td>0.970547</td>\n",
       "      <td>0.969486</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>10</td>\n",
       "      <td>0.971408</td>\n",
       "      <td>0.973724</td>\n",
       "      <td>0.972566</td>\n",
       "      <td>0.001158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>47.964628</td>\n",
       "      <td>1.465270</td>\n",
       "      <td>12.453813</td>\n",
       "      <td>0.768611</td>\n",
       "      <td>0.70</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.7, 'num_leaves': 128}</td>\n",
       "      <td>0.967570</td>\n",
       "      <td>0.969315</td>\n",
       "      <td>0.968443</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>11</td>\n",
       "      <td>0.969226</td>\n",
       "      <td>0.970743</td>\n",
       "      <td>0.969985</td>\n",
       "      <td>0.000758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>68.897867</td>\n",
       "      <td>3.880324</td>\n",
       "      <td>14.008325</td>\n",
       "      <td>0.313069</td>\n",
       "      <td>0.70</td>\n",
       "      <td>256</td>\n",
       "      <td>{'learning_rate': 0.7, 'num_leaves': 256}</td>\n",
       "      <td>0.949765</td>\n",
       "      <td>0.965600</td>\n",
       "      <td>0.957682</td>\n",
       "      <td>0.007918</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951527</td>\n",
       "      <td>0.968693</td>\n",
       "      <td>0.960110</td>\n",
       "      <td>0.008583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       98.779547      0.946638        14.711493        0.277883   \n",
       "1       94.529286      0.926894        13.868437        0.247973   \n",
       "2       68.146726      0.511424        11.636487        0.263768   \n",
       "3       58.402427      4.384872         9.723119        0.349699   \n",
       "4       28.826629      0.630166         7.994866        0.201128   \n",
       "5       26.649774      0.406546         8.610677        0.083260   \n",
       "6       35.504586      0.599904        10.101779        0.123615   \n",
       "7       34.976297      0.386149        10.287855        0.128522   \n",
       "8       50.079604      1.605808        12.735095        0.204151   \n",
       "9       70.000215      1.332182        16.317688        0.151828   \n",
       "10      47.964628      1.465270        12.453813        0.768611   \n",
       "11      68.897867      3.880324        14.008325        0.313069   \n",
       "\n",
       "    param_learning_rate  param_num_leaves  \\\n",
       "0                  0.02               256   \n",
       "1                  0.02               128   \n",
       "2                  0.02                64   \n",
       "3                  0.02                32   \n",
       "4                  0.40                32   \n",
       "5                  0.70                32   \n",
       "6                  0.40                64   \n",
       "7                  0.70                64   \n",
       "8                  0.40               128   \n",
       "9                  0.40               256   \n",
       "10                 0.70               128   \n",
       "11                 0.70               256   \n",
       "\n",
       "                                        params  split0_test_score  \\\n",
       "0   {'learning_rate': 0.02, 'num_leaves': 256}           0.987912   \n",
       "1   {'learning_rate': 0.02, 'num_leaves': 128}           0.986516   \n",
       "2    {'learning_rate': 0.02, 'num_leaves': 64}           0.985063   \n",
       "3    {'learning_rate': 0.02, 'num_leaves': 32}           0.983464   \n",
       "4     {'learning_rate': 0.4, 'num_leaves': 32}           0.983425   \n",
       "5     {'learning_rate': 0.7, 'num_leaves': 32}           0.981956   \n",
       "6     {'learning_rate': 0.4, 'num_leaves': 64}           0.980784   \n",
       "7     {'learning_rate': 0.7, 'num_leaves': 64}           0.979862   \n",
       "8    {'learning_rate': 0.4, 'num_leaves': 128}           0.976314   \n",
       "9    {'learning_rate': 0.4, 'num_leaves': 256}           0.968426   \n",
       "10   {'learning_rate': 0.7, 'num_leaves': 128}           0.967570   \n",
       "11   {'learning_rate': 0.7, 'num_leaves': 256}           0.949765   \n",
       "\n",
       "    split1_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0            0.987832         0.987872        0.000040                1   \n",
       "1            0.986479         0.986498        0.000018                2   \n",
       "2            0.984973         0.985018        0.000045                3   \n",
       "3            0.983290         0.983377        0.000087                4   \n",
       "4            0.983252         0.983339        0.000087                5   \n",
       "5            0.981920         0.981938        0.000018                6   \n",
       "6            0.981377         0.981080        0.000296                7   \n",
       "7            0.980125         0.979993        0.000131                8   \n",
       "8            0.979722         0.978018        0.001704                9   \n",
       "9            0.970547         0.969486        0.001061               10   \n",
       "10           0.969315         0.968443        0.000872               11   \n",
       "11           0.965600         0.957682        0.007918               12   \n",
       "\n",
       "    split0_train_score  split1_train_score  mean_train_score  std_train_score  \n",
       "0             0.990968            0.991069          0.991019         0.000050  \n",
       "1             0.988014            0.988184          0.988099         0.000085  \n",
       "2             0.985731            0.985950          0.985840         0.000110  \n",
       "3             0.983764            0.983920          0.983842         0.000078  \n",
       "4             0.984965            0.984797          0.984881         0.000084  \n",
       "5             0.983401            0.983681          0.983541         0.000140  \n",
       "6             0.982666            0.983824          0.983245         0.000579  \n",
       "7             0.981306            0.981954          0.981630         0.000324  \n",
       "8             0.979315            0.982140          0.980728         0.001412  \n",
       "9             0.971408            0.973724          0.972566         0.001158  \n",
       "10            0.969226            0.970743          0.969985         0.000758  \n",
       "11            0.951527            0.968693          0.960110         0.008583  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('LGBM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLGBM = joblib.load('LGBM_final.sav')\n",
    "yvalid = modelLGBM.predict(testN.drop('y',axis=1))\n",
    "(yvalid == testN['y']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultat du XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('XGB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelXGB = joblib.load('XGB_final.sav')\n",
    "yvalid = modelXGB.predict(test.drop('y',axis=1))\n",
    "(yvalid == test['y']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultat du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('NN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN = joblib.load('NN_final.sav')\n",
    "yvalid = modelNN.predict(testN.drop('y',axis=1))\n",
    "(yvalid == testN['y']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a file for submission\n",
    "\n",
    "Le modèle avec le meilleur score est le modèle `LightGBM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1598219,)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "xtest = pd.read_csv('testNormalized.csv')\n",
    "# Classify the provided test data\n",
    "ytest = modelLGBM.predict(xtest)\n",
    "print(ytest.shape)\n",
    "np.savetxt('ytest_final.csv', ytest, fmt = '%1.0d', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it's your turn. Good luck !  :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
